Data Science Playground

An intelligent, interactive web application designed to democratize data science. This tool provides a no-code environment for users to upload datasets, perform exploratory data analysis, and train machine learning models, all guided by AI-powered recommendations and interpretations.

Table of Contents
 * Key Features
 * Live Demo
 * Technologies Used
 * Setup and Installation
 * Usage
 * Project Structure
 * Testing
 * Future Improvements
 * License

Key Features
 * CSV Upload & Processing: Securely upload CSV files with built-in size limits and robust error handling.

 * AI-Powered Recommendations: Utilizes the Google Gemini API to analyze your dataset and suggest the most suitable machine learning model and features (X/Y variables) to start with.

 * Exploratory Data Analysis (EDA): A dedicated "Data Explorer" tab provides:
   * Dataset shape, statistical summaries, and data types.
   * Missing value analysis.
   * Interactive data previews with pagination.
   * Dynamic histograms to visualize the distribution of any column.

 * Supervised & Unsupervised Learning:
   * Supervised Models: Train a variety of regression and classification models, including Linear/Logistic Regression, Random Forest, Support Vector Machines (SVM), and Neural Networks.
   * Unsupervised Models: Perform K-Means clustering on your data, with dimensionality reduction via PCA or t-SNE for visualization.

 * Advanced Model Configuration:
   * Fine-tune model hyperparameters like learning rate and epochs.
   * Enable automated GridSearchCV for hyperparameter tuning on complex models.
   * Apply automated feature engineering (Polynomial Features) with a single click.
 * ðŸ“ˆ Interactive Visualizations: All model results are accompanied by interactive Plotly charts, such as scatter plots of predictions vs. actuals or cluster plots.

 * AI-Powered Interpretation: After training a model, receive a simple, one-paragraph explanation of what the results and performance metrics (MSE, Accuracy, Inertia) mean, generated by the Gemini API.


Technologies Used
 * Backend: Python
 * Web Framework: Streamlit
 * Machine Learning: Scikit-learn
 * Data Manipulation: Pandas, NumPy
 * Visualizations: Plotly Express
 * Generative AI: Google Gemini API
 * Environment Management: python-dotenv

Setup and Installation
Follow these steps to set up and run the project on your local machine.
1. Prerequisites
 * Python 3.9 or newer.
 * A Google Gemini API key. You can obtain one from Google AI Studio.

2. Clone the Repository
git clone https://github.com/AlexBiobelemo/Project-Nefertiti

3. Create a Virtual Environment
It is highly recommended to use a virtual environment to manage project dependencies.
# For Windows

python -m venv venv
venv\Scripts\Activate.ps1

# For macOS/Linux
python3 -m venv venv
source venv/bin/activate

4. Install Dependencies
Create a file named requirements.txt in the project's root directory and add the following lines:
streamlit
pandas
numpy
scikit-learn
plotly
plotly-express
google-generativeai
python-dotenv
joblib
textblob

Now, install these dependencies using pip:
pip install -r requirements.txt

5. Set Up Environment Variables
Create a file named .env in the root of your project directory. This file will securely store your API key.
Add your Gemini API key to the .env file like this:
GEMINI_API_KEY="your_actual_api_key_here"

Usage
Once the setup is complete, you can launch the Streamlit application with the following command:
streamlit run app.py

Your web browser should automatically open to the application's local address (usually http://localhost:8501).
Project Structure

.env                  # Stores environment variables like the API key

app.py                # Main Streamlit application script

test_app.py           # Unit tests for the application logic

requirements.txt      # Project dependencies

README.md             # You are here

TECHNICAL_DETAILS.md  # Technical nuances of the project


Testing
The project includes a suite of unit tests to ensure the core data processing and model training functions are working correctly. To run the tests, execute the following command in your terminal from the project's root directory:
python -m unittest test_app.py

Future Improvements
 * [ ] Support for more file formats (e.g., Excel, Parquet).
 * [ ] Expanded feature engineering options (e.g., custom transformations).
 * [ ] Time-series analysis and forecasting models.
 * [ ] Model saving/loading and comparison features.
 * [ ] User authentication and project saving.
License

This project is licensed under the MIT License. See the LICENSE file for more details.
